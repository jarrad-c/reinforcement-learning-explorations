{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GridSearch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKgLqznAahxI",
        "colab_type": "code",
        "outputId": "09b1e260-c17a-4d43-daad-820d645dd1a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Used in Google Colab\n",
        "#Install Dependencies\n",
        "!pip install tensorflow==1.15\n",
        "!pip install stable-baselines\n",
        "!pip install gym-retro atari_py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.15 in /usr/local/lib/python3.6/dist-packages (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.17.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.11.2)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.1.8)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.10.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.15.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.33.6)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (42.0.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (0.16.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.8.0)\n",
            "Requirement already satisfied: stable-baselines in /usr/local/lib/python3.6/dist-packages (2.2.1)\n",
            "Requirement already satisfied: tensorflow>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (3.1.2)\n",
            "Requirement already satisfied: mpi4py in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (3.0.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (0.25.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (0.3.1.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (0.9.0)\n",
            "Requirement already satisfied: zmq in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (0.0.0)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.10.9 in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (0.15.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (7.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (1.3.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (1.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (4.1.2.30)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (0.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (0.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (4.28.1)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (3.38.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (1.17.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (0.1.8)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (1.0.8)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (1.15.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (0.33.6)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (0.8.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (3.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (1.11.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (3.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (1.15.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (1.15.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines) (2.6.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines) (2.4.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines) (2018.9)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from zmq->stable-baselines) (17.0.0)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.10.9->stable-baselines) (1.3.2)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.10.9->stable-baselines) (0.2.6)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.10.9->stable-baselines) (4.3.0)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->stable-baselines) (2.3.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow>=1.5.0->stable-baselines) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow>=1.5.0->stable-baselines) (42.0.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.5.0->stable-baselines) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.5.0->stable-baselines) (0.16.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym[atari,classic_control]>=0.10.9->stable-baselines) (0.16.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow; extra == \"atari\"->gym[atari,classic_control]>=0.10.9->stable-baselines) (0.46)\n",
            "Requirement already satisfied: gym-retro in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: atari_py in /usr/local/lib/python3.6/dist-packages (0.2.6)\n",
            "Requirement already satisfied: pyglet==1.*,>=1.3.2 in /usr/local/lib/python3.6/dist-packages (from gym-retro) (1.3.2)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from gym-retro) (0.15.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from atari_py) (1.17.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari_py) (1.12.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet==1.*,>=1.3.2->gym-retro) (0.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.3.3)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (4.1.2.30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06KNF6qdaivz",
        "colab_type": "code",
        "outputId": "900f40d3-8f40-4c1f-b296-5824ece32262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "#Import ROMs\n",
        "!python -m retro.import ."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importing 20 potential games...\n",
            "Importing StreetFighterIISpecialChampionEdition-Genesis\n",
            "Importing MortalKombat3-Genesis\n",
            "Imported 2 games\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw8k9MM-apbD",
        "colab_type": "code",
        "outputId": "c52a4cff-0883-4862-e6f9-ef5d2ca93513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "import gym\n",
        "import retro\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from stable_baselines.common.policies import MlpPolicy, CnnPolicy\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines import PPO2, A2C, results_plotter, ACKTR, ACER\n",
        "from stable_baselines.bench import Monitor\n",
        "from stable_baselines.results_plotter import load_results, ts2xy\n",
        "from gym.wrappers.gray_scale_observation import GrayScaleObservation\n",
        "from gym.wrappers.resize_observation import ResizeObservation\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F9NhWdXa2sZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, env, num_steps=10):\n",
        "  \"\"\"\n",
        "  Evaluate a RL agent\n",
        "  :param model: (BaseRLModel object) the RL Agent\n",
        "  :param num_steps: (int) number of timesteps to evaluate it\n",
        "  :return: (float) Mean reward for the last 100 episodes\n",
        "  \"\"\"\n",
        "  episode_rewards = [0.0]\n",
        "  obs = env.reset()\n",
        "  for i in range(num_steps):\n",
        "      # _states are only useful when using LSTM policies\n",
        "      action, _states = model.predict(obs)\n",
        "      # here, action, rewards and dones are arrays\n",
        "      # because we are using vectorized env\n",
        "      obs, rewards, dones, info = env.step(action)\n",
        "      \n",
        "      # Stats\n",
        "      episode_rewards[-1] += rewards[0]\n",
        "      if dones[0]:\n",
        "          obs = env.reset()\n",
        "          episode_rewards.append(0.0)\n",
        "  # Compute mean reward for the last 100 episodes\n",
        "  mean_100ep_reward = round(np.mean(episode_rewards[-100:]), 1)\n",
        "  print(\"Mean reward:\", mean_100ep_reward, \"Num episodes:\", len(episode_rewards))\n",
        "  \n",
        "  return mean_100ep_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8ih4Mzfa_ME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_mean_reward, n_steps = -np.inf, 0\n",
        "\n",
        "def callback(_locals, _globals):\n",
        "    \"\"\"\n",
        "    Callback called at each step (for DQN an others) or after n steps (see ACER or PPO2)\n",
        "    :param _locals: (dict)\n",
        "    :param _globals: (dict)\n",
        "    \"\"\"\n",
        "    global n_steps, best_mean_reward\n",
        "    # Print stats every 1000 calls\n",
        "    if (n_steps + 1) % 1000 == 0:\n",
        "        # Evaluate policy training performance\n",
        "        x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
        "        if len(x) > 0:\n",
        "            mean_reward = np.mean(y[-100:])\n",
        "            #print(x[-1], 'timesteps')\n",
        "            #print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(best_mean_reward, mean_reward))\n",
        "\n",
        "            # New best model, you could save the agent here\n",
        "            if mean_reward > best_mean_reward:\n",
        "                best_mean_reward = mean_reward\n",
        "                # Example for saving best model\n",
        "                #print(\"Saving new best model\")\n",
        "                _locals['self'].save(log_dir + 'best_model.pkl')\n",
        "    n_steps += 1\n",
        "    # Returning False will stop training early\n",
        "    return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLYTUZVsbB_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_dir = \"/tmp/gym/\"\n",
        "\n",
        "def create_env(env_id):\n",
        "  os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "  # Create and wrap the environment\n",
        "  env = retro.make(game=env_id)\n",
        "\n",
        "  # wrap the environment - preprocesssing\n",
        "  # Originally the shape is 84 84\n",
        "  shape = (42,42)\n",
        "  env = ResizeObservation(env, shape)  \n",
        "  env = GrayScaleObservation(env)\n",
        "\n",
        "  # Logs will be saved in log_dir/monitor.csv\n",
        "  env = Monitor(env, log_dir, allow_early_resets=True)\n",
        "\n",
        "  # Because we use parameter noise, we should use a MlpPolicy with layer normalization\n",
        "  return DummyVecEnv([lambda: env])  # The algorithms require a vectorized environment to run"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRwnz372cReZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Change the 2 dictionaries and num_steps_learn below to change parameters of search\n",
        "#Dictionaries are hyperparameters, num_steps_learn is length of test\n",
        "params_grid_PPO2 = {\n",
        "    'policy':['MlpPolicy'],\n",
        "    'gamma':[0.5],\n",
        "    'n_steps':[256],\n",
        "    'ent_coef':[0.001],\n",
        "    'learning_rate':[.001],\n",
        "    'vf_coef':[0.75]\n",
        "}\n",
        "\n",
        "params_grid_A2C = {\n",
        "    'policy':['MlpPolicy'],\n",
        "    'gamma':[0.99],\n",
        "    'n_steps':[5, 10],\n",
        "    'ent_coef':[.01, 0.0125],\n",
        "    'learning_rate':[0.0005, 0.00025],\n",
        "    'vf_coef':[0.15, 0.35]\n",
        "}\n",
        "\n",
        "num_steps_learn = 100000\n",
        "num_steps_evaluate = int(num_steps_learn/20)\n",
        "env = create_env('MortalKombat3-Genesis')\n",
        "#Grid Search for hyperparam tuning\n",
        "def GridSearch(model, env):\n",
        "  num_tests = 0\n",
        "  best_reward = 0\n",
        "  best_params = {}\n",
        "  for pol in params_grid_PPO2['policy']:\n",
        "    for gam in params_grid_PPO2['gamma']:\n",
        "      for n in params_grid_PPO2['n_steps']:\n",
        "        for ent in params_grid_PPO2['ent_coef']:\n",
        "          for rate in params_grid_PPO2['learning_rate']:\n",
        "            for vf in params_grid_PPO2['vf_coef']:\n",
        "              num_tests += 1\n",
        "              print('On test #' + str(num_tests))\n",
        "              print('Pol: ' + pol)\n",
        "              print('Gam: ' + str(gam))\n",
        "              print('N: ' + str(n))\n",
        "              print('Ent: ' + str(ent))\n",
        "              print('Rate: ' + str(rate))\n",
        "              print('Vf: ' + str(vf))\n",
        "              if model == 'PPO2':\n",
        "                model = PPO2(pol, env, gamma=gam, n_steps=n, ent_coef=ent, learning_rate=rate, vf_coef=vf, verbose=0)\n",
        "              elif model == 'A2C':\n",
        "                model = A2C(pol, env, gamma=gam, n_steps=n, ent_coef=ent, learning_rate=rate, vf_coef=vf, verbose=0)\n",
        "              mean_reward_before_train = evaluate(model, env, num_steps=num_steps_evaluate)\n",
        "              model.learn(total_timesteps=int(num_steps_learn), callback=callback)\n",
        "              mean_reward_after_train = evaluate(model, env, num_steps=num_steps_evaluate)\n",
        "              if mean_reward_after_train > best_reward:\n",
        "                best_reward = mean_reward_after_train\n",
        "                best_params = {\n",
        "                    'policy':pol,\n",
        "                    'gamma':gam,\n",
        "                    'n_steps':n,\n",
        "                    'ent_coef':ent,\n",
        "                    'learning_rate':rate,\n",
        "                    'vf_coef':vf\n",
        "                }\n",
        "  return best_params, best_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATRqArgNbY5r",
        "colab_type": "code",
        "outputId": "19c32699-56a1-42de-e4de-d7fd39ad4e01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "model = PPO2('MlpPolicy', env, gamma=0.5, n_steps=256, ent_coef=0.001, learning_rate=0.001, vf_coef=0.75)\n",
        "mean_reward_before_train = evaluate(model, env, num_steps=5000)\n",
        "model.learn(total_timesteps=int(50000), callback=callback)\n",
        "mean_reward_after_train = evaluate(model, env, num_steps=5000)\n",
        "print('Reward Before: ' + str(mean_reward_before_train))\n",
        "print('Reward After 1 million time steps: ' + str(mean_reward_after_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean reward: 157.0 Num episodes: 2\n",
            "Mean reward: 130.0 Num episodes: 2\n",
            "Reward Before: 157.0\n",
            "Reward After 1 million time steps: 130.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cX44bSHDj9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK3ZNLQNbhCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the agent\n",
        "import os\n",
        "#os.mkdir('models')\n",
        "model.save(\"models/ppo2_mk3\")\n",
        "del model  # delete trained model to demonstrate loading"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "700Ovntmb9KF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = PPO2.load(\"models/ppo2_mk3\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOj62g94b_3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVNvCI8EcCTu",
        "colab_type": "code",
        "outputId": "80101801-9ada-4837-b14c-7f436d628492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "import gym\n",
        "from stable_baselines.common.vec_env import VecVideoRecorder\n",
        "\n",
        "env_id = 'MortalKombat3-Genesis'\n",
        "video_folder = 'logs/videos/'\n",
        "video_length = 100000\n",
        "\n",
        "#env = create_env('MortalKombat3-Genesis')\n",
        "\n",
        "obs = env.reset()\n",
        "\n",
        "# Record the video starting at the first step\n",
        "env = VecVideoRecorder(env, video_folder,\n",
        "                       record_video_trigger=lambda x: x == 0, video_length=video_length,\n",
        "                       name_prefix=\"random-agent-{}\".format(env_id))\n",
        "\n",
        "env.reset()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 64,  55,  64, ...,  64,  55,  64],\n",
              "        [ 64,  55,  64, ...,  64,  55,  64],\n",
              "        [ 55,  56,  94, ..., 106,  56,  55],\n",
              "        ...,\n",
              "        [ 44,  43,  45, ...,  45,  43,  44],\n",
              "        [ 46,  45,  44, ...,  44,  45,  46],\n",
              "        [ 47,  43,  46, ...,  46,  43,  47]]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysqt4QlUcHkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import imageio\n",
        "\n",
        "model.set_env(env)\n",
        "\n",
        "images = []\n",
        "obs = model.env.reset()\n",
        "img = model.env.render(mode='rgb_array')\n",
        "done = False\n",
        "while not done:\n",
        "    images.append(img)\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, done ,_ = model.env.step(action)\n",
        "    img = model.env.render(mode='rgb_array')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpWKVDFhcKIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imageio.mimsave('stf2_ac2_300kEpisodes.gif', [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUba2L_5J7vl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}